{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this file is to take final results of each experiment and conduct error analysis\n",
    "\n",
    "- We want to know the Overall Accuracy and LD\n",
    "- We also want to know the same values per key field\n",
    "\n",
    "What needs to be done:\n",
    "- First need to merge jsonl files and initial metadata.jsonl to be able to see the file name is it and trace back\n",
    "- We then analyse the merged jsonl file\n",
    "- This should be outputed as csv files and images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install sklearn\n",
    "!pip install Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Levenshtein import distance\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the results.jsonl file without file names yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the setup for which you want to conduct error analysis\n",
    "setup_name = \"setup_12\"  # Change this to the desired setup name\n",
    "\n",
    "# Directory paths\n",
    "final_results_dir = \"final_results\"\n",
    "setup_dir = os.path.join(final_results_dir, setup_name)\n",
    "results_json_dir = os.path.join(setup_dir, \"results_json\")\n",
    "merged_jsonl_dir = os.path.join(setup_dir, \"merged_jsonl\")\n",
    "\n",
    "# File paths\n",
    "predictions_file = os.path.join(results_json_dir, f'predictions_{setup_name}.json')\n",
    "ground_truths_file = os.path.join(results_json_dir, f'ground_truths_{setup_name}.json')\n",
    "results_file = os.path.join(merged_jsonl_dir, 'results.jsonl')\n",
    "\n",
    "# Load predictions\n",
    "with open(predictions_file, 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "# Load ground truths\n",
    "with open(ground_truths_file, 'r') as f:\n",
    "    ground_truths = json.load(f)\n",
    "\n",
    "# Print the setup name\n",
    "print(f\"Setup: {setup_name}\\n\")\n",
    "\n",
    "# Open the results file\n",
    "with open(results_file, 'w') as f:\n",
    "    # Compare predictions and ground truths\n",
    "    for i, (pred, truth) in enumerate(zip(predictions, ground_truths)):\n",
    "        print(f\"Sample {i + 1}:\")\n",
    "        print(\"Ground Truth:\", json.dumps(truth, indent=2))\n",
    "        print(\"Prediction:\", json.dumps(pred, indent=2))\n",
    "        \n",
    "        correct_count = 0\n",
    "        total_count = len(truth)\n",
    "        \n",
    "        for key in truth:\n",
    "            if key in pred and pred[key] == truth[key]:\n",
    "                print(f\"Correct prediction for {key}: {pred[key]}\")\n",
    "                correct_count += 1\n",
    "            else:\n",
    "                print(f\"Incorrect prediction for {key}: predicted {pred.get(key)}, actual {truth[key]}\")\n",
    "        \n",
    "        accuracy = correct_count / total_count\n",
    "        print(f\"Accuracy for this sample: {accuracy}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # Write the results for this sample to the JSONL file\n",
    "        result = {\n",
    "            \"file_name\": f\"Sample {i + 1}\",\n",
    "            \"ground_truth\": json.dumps(truth),\n",
    "            \"predictions\": json.dumps(pred),\n",
    "            # \"accuracy\": accuracy\n",
    "        }\n",
    "        f.write(json.dumps(result) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create merged jsonl file with names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "metadata_dir = \"metadata\"\n",
    "\n",
    "# File paths\n",
    "metadata_file = os.path.join(metadata_dir, 'metadata.jsonl')\n",
    "merged_file = os.path.join(merged_jsonl_dir, 'merged.jsonl')\n",
    "\n",
    "# Load results from the first step\n",
    "with open(results_file, 'r') as f:\n",
    "    results = [json.loads(line) for line in f]\n",
    "\n",
    "# Load metadata\n",
    "with open(metadata_file, 'r') as f:\n",
    "    metadata = [json.loads(line) for line in f]\n",
    "\n",
    "# Function to check if two entries match based on available keys\n",
    "def entries_match(entry1, entry2):\n",
    "    for key in entry1:\n",
    "        if key in entry2 and entry1[key] != entry2[key]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Print the setup name\n",
    "print(f\"Setup: {setup_name}\\n\")\n",
    "\n",
    "# Open the merged file for writing\n",
    "with open(merged_file, 'w') as f:\n",
    "    # Iterate over the results\n",
    "    for result in results:\n",
    "        # Parse the ground truth from the result\n",
    "        result_gt = json.loads(result['ground_truth'])\n",
    "\n",
    "        # Look for matching metadata entries\n",
    "        matches = [meta for meta in metadata if entries_match(result_gt, json.loads(meta['ground_truth'])['gt_parse'])]\n",
    "\n",
    "        if len(matches) > 1:\n",
    "            print(f\"Multiple matches found for result: {result['file_name']}\")\n",
    "            for match in matches:\n",
    "                print(f\"File name: {match['file_name']}\")\n",
    "                print(f\"Invoice values: {json.loads(match['ground_truth'])['gt_parse']}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "        if matches:\n",
    "            # If at least one match is found, update the file_name from the first match\n",
    "            matched_entry = result.copy()\n",
    "            matched_entry['file_name'] = matches[0]['file_name']\n",
    "\n",
    "            # Write the matched entry to the merged file\n",
    "            f.write(json.dumps(matched_entry) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see if metadata file contains any duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# File paths\n",
    "metadata_dir = \"metadata\"\n",
    "metadata_file = os.path.join(metadata_dir, 'metadata.jsonl')\n",
    "\n",
    "# Load metadata\n",
    "with open(metadata_file, 'r') as f:\n",
    "    metadata = [json.loads(line) for line in f]\n",
    "\n",
    "# Dictionary to store ground truths and their corresponding file names\n",
    "ground_truth_to_files = {}\n",
    "\n",
    "# Populate the dictionary\n",
    "for entry in metadata:\n",
    "    gt_string = entry['ground_truth']\n",
    "    gt = json.loads(gt_string)['gt_parse']\n",
    "    serialized_gt = json.dumps(gt, sort_keys=True)  # Convert to string to make it hashable and ensure consistent ordering\n",
    "\n",
    "    if serialized_gt in ground_truth_to_files:\n",
    "        ground_truth_to_files[serialized_gt].append(entry['file_name'])\n",
    "    else:\n",
    "        ground_truth_to_files[serialized_gt] = [entry['file_name']]\n",
    "\n",
    "# Filter out entries with only one file name (i.e., unique ground truths)\n",
    "duplicate_gt_entries = {k: v for k, v in ground_truth_to_files.items() if len(v) > 1}\n",
    "\n",
    "# Print the results\n",
    "for gt, file_names in duplicate_gt_entries.items():\n",
    "    print(f\"Ground Truth: {gt}\")\n",
    "    for file_name in file_names:\n",
    "        print(f\"File name: {file_name}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Print the setup name\n",
    "print(f\"Setup: {setup_name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall and per field values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "setup_dir = os.path.join(final_results_dir, setup_name)\n",
    "merged_jsonl_dir = os.path.join(setup_dir, \"merged_jsonl\")\n",
    "report_dir = os.path.join(setup_dir, \"report\")\n",
    "images_dir = os.path.join(setup_dir, \"images\")\n",
    "\n",
    "# Ensure the images directory exists\n",
    "if not os.path.exists(images_dir):\n",
    "    os.makedirs(images_dir)\n",
    "\n",
    "# File paths\n",
    "merged_file = os.path.join(merged_jsonl_dir, 'merged.jsonl')\n",
    "\n",
    "# Load merged data\n",
    "with open(merged_file, 'r') as f:\n",
    "    merged_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Initialize lists to store F1 scores and Levenshtein distances for each invoice\n",
    "f1_scores = []\n",
    "lev_distances = []\n",
    "\n",
    "# Initialize dictionaries to store field-level F1 scores and Levenshtein distances\n",
    "field_f1_scores = {}\n",
    "field_lev_distances = {}\n",
    "field_TP_FP_FN = {}  # Store TP, FP, FN for each field\n",
    "\n",
    "# Iterate over the merged data to calculate metrics\n",
    "for entry in merged_data:\n",
    "    ground_truth = json.loads(entry['ground_truth'])\n",
    "    predictions = json.loads(entry['predictions'])\n",
    "    \n",
    "    # Calculate TP, FP, and FN for F1 score\n",
    "    TP = sum(1 for key in ground_truth if key in predictions and ground_truth[key] == predictions[key])\n",
    "    FP = sum(1 for key in predictions if key not in ground_truth or (key in ground_truth and ground_truth[key] != predictions[key]))\n",
    "    FN = sum(1 for key in ground_truth if key not in predictions)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score for this entry\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    # Calculate field-level F1 scores and Levenshtein distances\n",
    "    for key in ground_truth:\n",
    "        if key not in field_f1_scores:\n",
    "            field_f1_scores[key] = []\n",
    "            field_lev_distances[key] = []\n",
    "            field_TP_FP_FN[key] = {\"TP\": 0, \"FP\": 0, \"FN\": 0}\n",
    "        \n",
    "        # Calculate field-level F1 score\n",
    "        if key in predictions:\n",
    "            if ground_truth[key] == predictions[key]:\n",
    "                field_f1 = 1\n",
    "                field_TP_FP_FN[key][\"TP\"] += 1\n",
    "            else:\n",
    "                field_f1 = 0\n",
    "                field_TP_FP_FN[key][\"FP\"] += 1\n",
    "        else:\n",
    "            field_f1 = 0\n",
    "            field_TP_FP_FN[key][\"FN\"] += 1\n",
    "        field_f1_scores[key].append(field_f1)\n",
    "        \n",
    "        # Calculate field-level Levenshtein distance\n",
    "        lev_distance = distance(ground_truth[key], predictions.get(key, \"\"))\n",
    "        field_lev_distances[key].append(lev_distance)\n",
    "    \n",
    "    # Calculate overall Levenshtein distance for this entry\n",
    "    num_fields_in_gt = len(ground_truth)\n",
    "    avg_lev_distance = sum(distance(ground_truth[key], predictions.get(key, \"\")) for key in ground_truth) / num_fields_in_gt if num_fields_in_gt > 0 else 0\n",
    "    lev_distances.append(avg_lev_distance)\n",
    "\n",
    "# Calculate overall metrics\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "mean_lev_distance = np.mean(lev_distances)\n",
    "\n",
    "# Print overall metrics\n",
    "print(f\"Overall F1 Score: {mean_f1_score:.4f} (Based on {len(merged_data)} invoices)\")\n",
    "print(f\"Overall Levenshtein Distance: {mean_lev_distance:.4f} (Based on {len(merged_data)} invoices)\")\n",
    "\n",
    "# Print field-level metrics\n",
    "print(\"\\nField-Level F1 Scores:\")\n",
    "for key, scores in field_f1_scores.items():\n",
    "    TP = field_TP_FP_FN[key][\"TP\"]\n",
    "    FP = field_TP_FP_FN[key][\"FP\"]\n",
    "    FN = field_TP_FP_FN[key][\"FN\"]\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    print(f\"{key}: {np.mean(scores):.4f} (TP = {TP}, FP = {FP}, FN = {FN}, precision = {precision:.4f}, recall = {recall:.4f}, Total: {TP + FP + FN} invoices)\")\n",
    "\n",
    "print(\"\\nField-Level Levenshtein Distances:\")\n",
    "for key, distances in field_lev_distances.items():\n",
    "    print(f\"{key}: {np.mean(distances):.4f}\")\n",
    "\n",
    "# Define the order of keys for plotting\n",
    "ordered_keys = [\n",
    "    \"vendor_name\",\n",
    "    \"invoice_date\",\n",
    "    \"invoice_number\",\n",
    "    \"total_amount\",\n",
    "    \"charge_period_start_date\",\n",
    "    \"charge_period_end_date\",\n",
    "    \"mpan\",\n",
    "    \"account_number\"\n",
    "]\n",
    "\n",
    "# Calculate accuracy per key\n",
    "accuracy_per_key = pd.Series({key: field_TP_FP_FN[key][\"TP\"] / (field_TP_FP_FN[key][\"TP\"] + field_TP_FP_FN[key][\"FP\"] + field_TP_FP_FN[key][\"FN\"]) for key in ordered_keys})\n",
    "\n",
    "# Calculate average Levenshtein distance per key\n",
    "lev_distance_per_key = pd.Series({key: np.mean(field_lev_distances[key]) for key in ordered_keys})\n",
    "\n",
    "# Plot the accuracy per key, adjust the plot, and save as image\n",
    "accuracy_per_key.plot(kind='bar', figsize=(12, 7))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy per Key')\n",
    "plt.xticks(rotation=45)  # Rotate the key names by 45 degrees\n",
    "plt.tight_layout()  # Adjust the layout to ensure everything fits\n",
    "plt.savefig(os.path.join(images_dir, f'accuracy_per_key_{setup_name}.png'))\n",
    "plt.show()\n",
    "\n",
    "# Plot the Levenshtein distance per key, adjust the plot, and save as image\n",
    "lev_distance_per_key.plot(kind='bar', figsize=(12, 7))\n",
    "plt.ylabel('Average Levenshtein Distance')\n",
    "plt.title('Average Levenshtein Distance per Field')\n",
    "plt.xticks(rotation=45)  # Rotate the key names by 45 degrees\n",
    "plt.tight_layout()  # Adjust the layout to ensure everything fits\n",
    "plt.savefig(os.path.join(images_dir, f'lev_distance_per_key_{setup_name}.png'))\n",
    "plt.show()\n",
    "\n",
    "# Create DataFrames to store the results for each sheet\n",
    "df_overall = pd.DataFrame(columns=[\"Metric\", \"Value\", \"Description\"])\n",
    "df_field_f1 = pd.DataFrame(columns=[\"Field\", \"F1 Score\", \"TP\", \"FP\", \"FN\", \"Precision\", \"Recall\", \"Total Invoices\"])\n",
    "df_field_ld = pd.DataFrame(columns=[\"Field\", \"Levenshtein Distance\"])\n",
    "\n",
    "# Add overall metrics to the df_overall DataFrame\n",
    "df_overall.loc[len(df_overall)] = [\"Overall F1 Score\", f\"{mean_f1_score:.4f}\", f\"Based on {len(merged_data)} invoices\"]\n",
    "df_overall.loc[len(df_overall)] = [\"Overall Levenshtein Distance\", f\"{mean_lev_distance:.4f}\", f\"Based on {len(merged_data)} invoices\"]\n",
    "\n",
    "# Add field-level F1 scores to the df_field_f1 DataFrame\n",
    "for key, scores in field_f1_scores.items():\n",
    "    TP = field_TP_FP_FN[key][\"TP\"]\n",
    "    FP = field_TP_FP_FN[key][\"FP\"]\n",
    "    FN = field_TP_FP_FN[key][\"FN\"]\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    value = f\"{np.mean(scores):.4f}\"\n",
    "    df_field_f1.loc[len(df_field_f1)] = [key, value, TP, FP, FN, f\"{precision:.4f}\", f\"{recall:.4f}\", TP + FP + FN]\n",
    "\n",
    "# Add field-level Levenshtein distances to the df_field_ld DataFrame\n",
    "for key, distances in field_lev_distances.items():\n",
    "    value = f\"{np.mean(distances):.4f}\"\n",
    "    df_field_ld.loc[len(df_field_ld)] = [key, value]\n",
    "\n",
    "# Define the directory to save the report\n",
    "report_dir = os.path.join(setup_dir, \"report\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(report_dir):\n",
    "    os.makedirs(report_dir)\n",
    "\n",
    "# Save the DataFrames to different sheets of an Excel file\n",
    "report_file_path = os.path.join(report_dir, f\"metrics_{setup_name}.xlsx\")\n",
    "with pd.ExcelWriter(report_file_path, engine='openpyxl') as writer:\n",
    "    df_overall.to_excel(writer, sheet_name='Overall', index=False)\n",
    "    df_field_f1.to_excel(writer, sheet_name='Field level F1 score', index=False)\n",
    "    df_field_ld.to_excel(writer, sheet_name='Field level LD', index=False)\n",
    "\n",
    "print(f\"Metrics saved to {report_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoice level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Levenshtein import distance\n",
    "\n",
    "# Directory paths\n",
    "setup_dir = os.path.join(final_results_dir, setup_name)\n",
    "merged_jsonl_dir = os.path.join(setup_dir, \"merged_jsonl\")\n",
    "csv_dir = os.path.join(setup_dir, \"csv\")\n",
    "\n",
    "# Ensure the csv directory exists\n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)\n",
    "\n",
    "# File paths\n",
    "merged_file = os.path.join(merged_jsonl_dir, 'merged.jsonl')\n",
    "results_file = os.path.join(csv_dir, 'results.csv')\n",
    "\n",
    "# Load merged data\n",
    "with open(merged_file, 'r') as f:\n",
    "    merged_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Get all possible keys\n",
    "all_keys = set()\n",
    "for entry in merged_data:\n",
    "    ground_truth = json.loads(entry['ground_truth'])\n",
    "    predictions = json.loads(entry['predictions'])\n",
    "    all_keys.update(ground_truth.keys(), predictions.keys())\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "results = {key: [] for key in all_keys}\n",
    "results[\"file_name\"] = []\n",
    "results[\"overall_f1\"] = []\n",
    "results[\"overall_lev_distance\"] = []\n",
    "\n",
    "# For each key, initialize a list to store its Levenshtein distance\n",
    "for key in all_keys:\n",
    "    results[f\"{key}_lev_distance\"] = []\n",
    "\n",
    "print(\"Results saved to setup directory:\", setup_dir)\n",
    "\n",
    "# Evaluate each invoice\n",
    "for entry in merged_data:\n",
    "    print(f\"File name: {entry['file_name']}\")\n",
    "    \n",
    "    ground_truth = json.loads(entry['ground_truth'])\n",
    "    predictions = json.loads(entry['predictions'])\n",
    "    \n",
    "    print(\"Ground Truth:\", json.dumps(ground_truth, indent=2))\n",
    "    print(\"Prediction:\", json.dumps(predictions, indent=2))\n",
    "    \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    total_lev_distance = 0\n",
    "    \n",
    "    for key in all_keys:\n",
    "        gt_value = str(ground_truth.get(key, \"\"))\n",
    "        pred_value = str(predictions.get(key, \"\"))\n",
    "        \n",
    "        # Calculate Levenshtein distance for the key\n",
    "        lev_dist = distance(gt_value, pred_value)\n",
    "        total_lev_distance += lev_dist\n",
    "        results[f\"{key}_lev_distance\"].append(lev_dist)\n",
    "        \n",
    "        if key in ground_truth:\n",
    "            if gt_value == pred_value and gt_value:  # Both values are equal and not empty\n",
    "                print(f\"TP - Correct prediction for {key}: {pred_value}\")\n",
    "                results[key].append(\"TP\")\n",
    "                TP += 1\n",
    "            elif pred_value:  # Prediction has a value but doesn't match ground truth\n",
    "                print(f\"FP - Incorrect prediction for {key}: predicted {pred_value}, actual {gt_value}\")\n",
    "                results[key].append(\"FP\")\n",
    "                FP += 1\n",
    "            else:  # Ground truth has a value but prediction doesn't\n",
    "                print(f\"FN - Missing prediction for {key}: actual {gt_value}\")\n",
    "                results[key].append(\"FN\")\n",
    "                FN += 1\n",
    "        else:\n",
    "            if pred_value:  # Prediction has a value but key is not in ground truth\n",
    "                print(f\"FP - Useless prediction for {key}: predicted {pred_value}\")\n",
    "                results[key].append(\"FP\")\n",
    "                FP += 1\n",
    "            else:\n",
    "                results[key].append(np.nan)  # No value for this key in both ground truth and prediction\n",
    "\n",
    "    # Calculate overall F1 score for the invoice\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    avg_lev_distance = total_lev_distance / len(ground_truth)\n",
    "    \n",
    "    print(f\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
    "    print(f\"Overall F1 Score for this file: {f1}\")\n",
    "    print(f\"Overall Levenshtein Distance for this file: {avg_lev_distance}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    results[\"file_name\"].append(entry['file_name'])\n",
    "    results[\"overall_f1\"].append(f1)\n",
    "    results[\"overall_lev_distance\"].append(avg_lev_distance)\n",
    "\n",
    "# Convert the results to a DataFrame and save as a CSV file\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(results_file, index=False)\n",
    "\n",
    "# Print the first 5 rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key field level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Directory paths\n",
    "setup_dir = os.path.join(final_results_dir, setup_name)\n",
    "merged_jsonl_dir = os.path.join(setup_dir, \"merged_jsonl\")\n",
    "csv_dir = os.path.join(setup_dir, \"csv\")\n",
    "\n",
    "# File paths\n",
    "merged_file = os.path.join(merged_jsonl_dir, 'merged.jsonl')\n",
    "\n",
    "# Load merged results\n",
    "with open(merged_file, 'r') as f:\n",
    "    merged_results = [json.loads(line) for line in f]\n",
    "\n",
    "# Print the setup name\n",
    "print(f\"Setup: {setup_name}\\n\")\n",
    "\n",
    "# Specify the keys you are interested in\n",
    "keys_of_interest = [\"vendor_name\", \"invoice_date\", \"invoice_number\", \"total_amount\", \"charge_period_start_date\", \"charge_period_end_date\", \"mpan\", \"account_number\"]\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "results = {key: {\"file_name\": [], \"ground_truth\": [], \"prediction\": [], \"error_type\": []} for key in keys_of_interest}\n",
    "\n",
    "# Adjust pandas display settings\n",
    "pd.set_option('display.float_format', lambda x: '%.0f' % x)\n",
    "\n",
    "# Compare predictions and ground truths\n",
    "for result in merged_results:\n",
    "    ground_truth = json.loads(result['ground_truth'])\n",
    "    predictions = json.loads(result['predictions'])\n",
    "    \n",
    "    for key in keys_of_interest:\n",
    "        if key in ground_truth or key in predictions:\n",
    "            results[key][\"file_name\"].append(result['file_name'])\n",
    "            results[key][\"ground_truth\"].append(ground_truth.get(key, \"N/A\"))\n",
    "            \n",
    "            if key in predictions:\n",
    "                if predictions[key] == ground_truth.get(key):\n",
    "                    results[key][\"prediction\"].append(\"Correct\")\n",
    "                    results[key][\"error_type\"].append(\"TP\")\n",
    "                else:\n",
    "                    results[key][\"prediction\"].append(f\"{predictions[key]}\")\n",
    "                    results[key][\"error_type\"].append(\"FP\")\n",
    "            else:\n",
    "                results[key][\"prediction\"].append(\"Missing\")\n",
    "                results[key][\"error_type\"].append(\"FN\")\n",
    "\n",
    "# Convert the results to a DataFrame and save as a CSV file for each key\n",
    "for key in keys_of_interest:\n",
    "    df = pd.DataFrame(results[key])\n",
    "    results_file = os.path.join(csv_dir, f'results_{key}.csv')\n",
    "    df.to_csv(results_file, index=False)\n",
    "\n",
    "print(\"Results saved to setup directory:\", setup_dir)\n",
    "\n",
    "# Load the CSV file for each key and print the first 5 rows\n",
    "for key in keys_of_interest:\n",
    "    print(f\"Results for {key}:\")\n",
    "    results_file = os.path.join(csv_dir, f'results_{key}.csv')\n",
    "    results_df = pd.read_csv(results_file)\n",
    "    # Filter out rows where the prediction is \"Correct\"\n",
    "    error_rows = results_df.loc[results_df['prediction'] != 'Correct']\n",
    "    display(error_rows)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Provide a summary for each key\n",
    "    total_items = len(results_df)\n",
    "    errors = len(error_rows)\n",
    "    accuracy = (total_items - errors) / total_items\n",
    "    TP_count = len(results_df.loc[results_df['error_type'] == 'TP'])\n",
    "    FP_count = len(results_df.loc[results_df['error_type'] == 'FP'])\n",
    "    FN_count = len(results_df.loc[results_df['error_type'] == 'FN'])\n",
    "    \n",
    "    print(f\"Summary for {key}:\")\n",
    "    print(f\"Total items: {total_items}\")\n",
    "    print(f\"Number of errors: {errors}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"TP: {TP_count}\")\n",
    "    print(f\"FP: {FP_count}\")\n",
    "    print(f\"FN: {FN_count}\")\n",
    "    print(\"========================================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per vendor metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Directory paths\n",
    "setup_dir = os.path.join(final_results_dir, setup_name)\n",
    "merged_jsonl_dir = os.path.join(setup_dir, \"merged_jsonl\")\n",
    "\n",
    "# File paths\n",
    "merged_file = os.path.join(merged_jsonl_dir, 'merged.jsonl')\n",
    "vendors_file = os.path.join(merged_jsonl_dir, 'vendors.jsonl')\n",
    "\n",
    "# Load merged data\n",
    "with open(merged_file, 'r') as f:\n",
    "    merged_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Extract vendor names and write to vendors.jsonl\n",
    "with open(vendors_file, 'w') as vf:\n",
    "    for entry in merged_data:\n",
    "        ground_truth = json.loads(entry['ground_truth'])\n",
    "        vendor_name = ground_truth['vendor_name']\n",
    "        entry['vendor_name'] = vendor_name\n",
    "        vf.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Directory paths\n",
    "setup_dir = os.path.join(final_results_dir, setup_name)\n",
    "merged_jsonl_dir = os.path.join(setup_dir, \"merged_jsonl\")\n",
    "csv_dir = os.path.join(setup_dir, \"csv\")\n",
    "\n",
    "# Ensure the csv directory exists\n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)\n",
    "\n",
    "# File paths\n",
    "vendors_file = os.path.join(merged_jsonl_dir, 'vendors.jsonl')\n",
    "results_file = os.path.join(csv_dir, 'results_by_vendor.csv')\n",
    "\n",
    "# Load data from vendors.jsonl\n",
    "with open(vendors_file, 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Group data by vendor_name for easier processing\n",
    "grouped_data = {}\n",
    "for entry in data:\n",
    "    vendor_name = entry['vendor_name']\n",
    "    if vendor_name not in grouped_data:\n",
    "        grouped_data[vendor_name] = []\n",
    "    grouped_data[vendor_name].append(entry)\n",
    "\n",
    "# Initialize a dictionary to store results for each vendor\n",
    "vendor_results = {}\n",
    "\n",
    "# Process each vendor's data\n",
    "for vendor_name, vendor_data in grouped_data.items():\n",
    "    # Initialize dictionaries to store field-level F1 scores and overall F1 scores\n",
    "    field_f1_scores = {}\n",
    "    overall_f1_scores = []\n",
    "    field_TP_FP_FN = {}  # Store TP, FP, FN for each field\n",
    "\n",
    "    # Iterate over the vendor data to calculate metrics\n",
    "    for entry in vendor_data:\n",
    "        ground_truth = json.loads(entry['ground_truth'])\n",
    "        predictions = json.loads(entry['predictions'])\n",
    "        \n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        \n",
    "        # Calculate F1 scores for each field\n",
    "        for key in ground_truth:\n",
    "            if key not in field_f1_scores:\n",
    "                field_f1_scores[key] = []\n",
    "                field_TP_FP_FN[key] = {\"TP\": 0, \"FP\": 0, \"FN\": 0}\n",
    "            \n",
    "            # Check if prediction matches ground truth\n",
    "            if key in predictions:\n",
    "                if ground_truth[key] == predictions[key]:\n",
    "                    field_f1 = 1\n",
    "                    field_TP_FP_FN[key][\"TP\"] += 1\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    field_f1 = 0\n",
    "                    field_TP_FP_FN[key][\"FP\"] += 1\n",
    "                    FP += 1\n",
    "            else:\n",
    "                field_f1 = 0\n",
    "                field_TP_FP_FN[key][\"FN\"] += 1\n",
    "                FN += 1\n",
    "            field_f1_scores[key].append(field_f1)\n",
    "        \n",
    "        # Calculate overall F1 score for the invoice\n",
    "        precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "        recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        overall_f1_scores.append(f1)\n",
    "\n",
    "    # Store results for the vendor\n",
    "    vendor_results[vendor_name] = {\n",
    "        'invoices_count': len(vendor_data),\n",
    "        'field_f1_scores': field_f1_scores,\n",
    "        'overall_f1': np.mean(overall_f1_scores)\n",
    "    }\n",
    "\n",
    "# Convert the results to a DataFrame for easier saving and visualization\n",
    "results_list = []\n",
    "for vendor_name, metrics in vendor_results.items():\n",
    "    result = {\n",
    "        'vendor_name': vendor_name,\n",
    "        'invoices_count': metrics['invoices_count'],\n",
    "        'overall_f1': metrics['overall_f1']\n",
    "    }\n",
    "    for key, scores in metrics['field_f1_scores'].items():\n",
    "        result[f\"{key}_f1\"] = np.mean(scores)\n",
    "    results_list.append(result)\n",
    "\n",
    "df = pd.DataFrame(results_list)\n",
    "df.to_csv(results_file, index=False)\n",
    "\n",
    "print(f\"Results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "invoicenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
